{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ETL Pipeline for the Concentration of Banking Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing key packages\n",
    "\n",
    "### Importation Summary\n",
    "1. Web Scraping: \n",
    "<span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">bs4</span> + <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">re</span>\n",
    "\n",
    "2. Parsing: <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">lxml</span> +  <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">BeautifulSoup</span>\n",
    "\n",
    "3. Data Processing: <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">pandas</span> + <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">numpy</span> + <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">os</span> + <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">datetime</span>\n",
    "\n",
    "3. File Upload: <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">json</span> + <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">base64</span> + <span style=\"background-color:rgb(79, 78, 78); padding: 2px 5px; border-radius: 3px;\">requests</span>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #Handles HTTP requests to fetch web contents from the federal reserve\n",
    "from bs4 import BeautifulSoup #Parses HTML content for web web scraping. It extracts the quarterly bank data tables from the downloaded HTML pages. \n",
    "import re #performs string pattern and substitution. It sanitizes filenames\n",
    "from datetime import datetime #Manages date/time operations to generated quarterly ranges and formats timestamps for filenames. \n",
    "import os #Interfaces with the operating system. \n",
    "import pandas as pd #Data manipulation and analysis from the dataframes converted from the HTML file tables. Cleans and transforms data. Converts dataframes into csv files. \n",
    "import lxml #Optimal parser for beautiful soup. \n",
    "import numpy as np #Numerical computing and NaN handling. \n",
    "import json #Serializes and deserializes Python objects to JSON format. Formats payloads for Github API requests during file uploads. \n",
    "import base64 #Converts file content to base64 for Github API uploads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the relative paths to save HTML and CSV files\n",
    "html_dir=\"./html_files\"\n",
    "csv_dir=\"./csv_files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping & File Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quarterly dates generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['March 31, 2003',\n",
       " 'June 30, 2003',\n",
       " 'September 30, 2003',\n",
       " 'December 31, 2003',\n",
       " 'March 31, 2004',\n",
       " 'June 30, 2004',\n",
       " 'September 30, 2004',\n",
       " 'December 31, 2004',\n",
       " 'March 31, 2005',\n",
       " 'June 30, 2005',\n",
       " 'September 30, 2005',\n",
       " 'December 31, 2005',\n",
       " 'March 31, 2006',\n",
       " 'June 30, 2006',\n",
       " 'September 30, 2006',\n",
       " 'December 31, 2006',\n",
       " 'March 31, 2007',\n",
       " 'June 30, 2007',\n",
       " 'September 30, 2007',\n",
       " 'December 31, 2007',\n",
       " 'March 31, 2008',\n",
       " 'June 30, 2008',\n",
       " 'September 30, 2008',\n",
       " 'December 31, 2008',\n",
       " 'March 31, 2009',\n",
       " 'June 30, 2009',\n",
       " 'September 30, 2009',\n",
       " 'December 31, 2009',\n",
       " 'March 31, 2010',\n",
       " 'June 30, 2010',\n",
       " 'September 30, 2010',\n",
       " 'December 31, 2010',\n",
       " 'March 31, 2011',\n",
       " 'June 30, 2011',\n",
       " 'September 30, 2011',\n",
       " 'December 31, 2011',\n",
       " 'March 31, 2012',\n",
       " 'June 30, 2012',\n",
       " 'September 30, 2012',\n",
       " 'December 31, 2012',\n",
       " 'March 31, 2013',\n",
       " 'June 30, 2013',\n",
       " 'September 30, 2013',\n",
       " 'December 31, 2013',\n",
       " 'March 31, 2014',\n",
       " 'June 30, 2014',\n",
       " 'September 30, 2014',\n",
       " 'December 31, 2014',\n",
       " 'March 31, 2015',\n",
       " 'June 30, 2015',\n",
       " 'September 30, 2015',\n",
       " 'December 31, 2015',\n",
       " 'March 31, 2016',\n",
       " 'June 30, 2016',\n",
       " 'September 30, 2016',\n",
       " 'December 31, 2016',\n",
       " 'March 31, 2017',\n",
       " 'June 30, 2017',\n",
       " 'September 30, 2017',\n",
       " 'December 31, 2017',\n",
       " 'March 31, 2018',\n",
       " 'June 30, 2018',\n",
       " 'September 30, 2018',\n",
       " 'December 31, 2018',\n",
       " 'March 31, 2019',\n",
       " 'June 30, 2019',\n",
       " 'September 30, 2019',\n",
       " 'December 31, 2019',\n",
       " 'March 31, 2020',\n",
       " 'June 30, 2020',\n",
       " 'September 30, 2020',\n",
       " 'December 31, 2020',\n",
       " 'March 31, 2021',\n",
       " 'June 30, 2021',\n",
       " 'September 30, 2021',\n",
       " 'December 31, 2021',\n",
       " 'March 31, 2022',\n",
       " 'June 30, 2022',\n",
       " 'September 30, 2022',\n",
       " 'December 31, 2022',\n",
       " 'March 31, 2023',\n",
       " 'June 30, 2023',\n",
       " 'September 30, 2023',\n",
       " 'December 31, 2023',\n",
       " 'March 31, 2024',\n",
       " 'June 30, 2024',\n",
       " 'September 30, 2024',\n",
       " 'December 31, 2024',\n",
       " 'March 31, 2025',\n",
       " 'June 30, 2025',\n",
       " 'September 30, 2025',\n",
       " 'December 31, 2025']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Develops a list of quarterly end dates from a given start year up to the current year.\"\n",
    "def generate_quarterly_dates(start_year):\n",
    "    \"\"\"Generates a list of quarterly end dates from a given start year up to the current year.\"\"\"\n",
    "    end_year = datetime.now().year  # Current year\n",
    "    quarters = [\"March 31,\", \"June 30,\", \"September 30,\", \"December 31,\"]\n",
    "    formatted_dates = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for quarter in quarters:\n",
    "            formatted_dates.append(quarter + \" \" + str(year))\n",
    "    return formatted_dates\n",
    "\n",
    "generate_quarterly_dates(2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date to quarter conversion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Converts a date to a quarter.'''\n",
    "def date_to_quarter(month, day):\n",
    "    if month == 3 and day == 31:\n",
    "        return 'Q1'\n",
    "    elif month == 6 and day == 30:\n",
    "        return 'Q2'\n",
    "    elif month == 9 and day == 30:\n",
    "        return 'Q3'\n",
    "    elif month == 12 and day == 31:\n",
    "        return 'Q4'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Set example date by picking March 31\n",
    "date_to_quarter(3, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscrape, update and rename files function\n",
    "Webscrapes the Federal Reserve website for large commercial bank data, updates the files, \n",
    "and renames them according to a specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Downloads data, uploads to local directory, and renames files according to a specified format.\"\"\"\n",
    "def update_and_rename_files():\n",
    "    \"\"\"Downloads data, uploads to local directory, and renames files according to a specified format.\"\"\"\n",
    "    URL = \"https://www.federalreserve.gov/releases/lbr/\" #URL of the Federal Reserve website\n",
    "    quarterly_dates = generate_quarterly_dates(2003) #Recalling the function that generates a list of quarterly end dates\n",
    "    response = requests.get(URL) #Sending a GET request to the URL\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\") #Parsing the HTML content\n",
    "    all_links = soup.find_all('a', href=True) #Finding all 'a' elements with 'href' attribute\n",
    "    links = [(link['href'], link.text) for link in all_links if any(date in link.text for date in quarterly_dates)] #Extracting 'href' and 'text' attributes from 'a' elements\n",
    "\n",
    "    for href, date in links: #Looping through the extracted 'href' and 'text' attributes\n",
    "        full_url = URL + href #Constructing the full URL\n",
    "        table_response = requests.get(full_url) #Sending a GET request to the full URL\n",
    "        filename = re.sub(r'\\W+', '_', href) + \".html\" #Sanitizing the filename\n",
    "        date_obj = datetime.strptime(date, \"%B %d, %Y\") #Converting the 'text' attribute to a datetime object\n",
    "        datenum = date_obj.strftime(\"%Y%m%d\") #Converting the datetime object to a string\n",
    "        quarter = date_to_quarter(date_obj.month, date_obj.day) #Calling the function that converts a date to a quarter\n",
    "        new_filename = f'{datenum}_{quarter}_{date_obj.year}_large_commercial_banks.html' #Constructing the new filename\n",
    "        current_path = os.path.join(html_dir, filename) # Path to current file\n",
    "        new_path = os.path.join(html_dir, new_filename) # Path to new file\n",
    "\n",
    "        #Write the content of the respons to a file\n",
    "        with open(new_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(table_response.text)\n",
    "\n",
    "update_and_rename_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\html_files'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_dir.replace('/', '\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HTML file to CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20030930_Q3_2003_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20031231_Q4_2003_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20040331_Q1_2004_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20040630_Q2_2004_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20040930_Q3_2004_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20041231_Q4_2004_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20050331_Q1_2005_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20050630_Q2_2005_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20050930_Q3_2005_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20051231_Q4_2005_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20060331_Q1_2006_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20060630_Q2_2006_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20060930_Q3_2006_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20061231_Q4_2006_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20070331_Q1_2007_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20070630_Q2_2007_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20070930_Q3_2007_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20071231_Q4_2007_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20080331_Q1_2008_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20080630_Q2_2008_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20080930_Q3_2008_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20081231_Q4_2008_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20090331_Q1_2009_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20090630_Q2_2009_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20090930_Q3_2009_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20091231_Q4_2009_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20100331_Q1_2010_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20100630_Q2_2010_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20100930_Q3_2010_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20101231_Q4_2010_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20110331_Q1_2011_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20110630_Q2_2011_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20110930_Q3_2011_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20111231_Q4_2011_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20120331_Q1_2012_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20120630_Q2_2012_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20120930_Q3_2012_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20121231_Q4_2012_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20130331_Q1_2013_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20130630_Q2_2013_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20130930_Q3_2013_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20131231_Q4_2013_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20140331_Q1_2014_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20140630_Q2_2014_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20140930_Q3_2014_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20141231_Q4_2014_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20150331_Q1_2015_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20150630_Q2_2015_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20150930_Q3_2015_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20151231_Q4_2015_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20160331_Q1_2016_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20160630_Q2_2016_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20160930_Q3_2016_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20161231_Q4_2016_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20170331_Q1_2017_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20170630_Q2_2017_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20170930_Q3_2017_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20171231_Q4_2017_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20180331_Q1_2018_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20180630_Q2_2018_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20180930_Q3_2018_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20181231_Q4_2018_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20190331_Q1_2019_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20190630_Q2_2019_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20190930_Q3_2019_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20191231_Q4_2019_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20200331_Q1_2020_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20200630_Q2_2020_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20200930_Q3_2020_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20201231_Q4_2020_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20210331_Q1_2021_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20210630_Q2_2021_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20210930_Q3_2021_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20211231_Q4_2021_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20220331_Q1_2022_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20220630_Q2_2022_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20220930_Q3_2022_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20221231_Q4_2022_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20230331_Q1_2023_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20230630_Q2_2023_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20230930_Q3_2023_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20231231_Q4_2023_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20240331_Q1_2024_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20240630_Q2_2024_large_commercial_banks.csv has been saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\2697750471.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20240930_Q3_2024_large_commercial_banks.csv has been saved.\n"
     ]
    }
   ],
   "source": [
    "\" Reads a specific table from an HTML file.\"\n",
    "def read_specific_table(html_file): \n",
    "    with open(html_file, 'r', encoding='utf8') as file:  # It's a good practice to define encoding\n",
    "        contents = file.read()\n",
    "\n",
    "        # Use BeautifulSoup to parse the HTML content\n",
    "        soup = BeautifulSoup(contents, 'html.parser')  # You can switch back to 'lxml' or use 'html.parser'\n",
    "\n",
    "        '''Create OR statements to account for differences in the attributes in the html code for the html files'''\n",
    "        def matches_table_attribute(tag):\n",
    "            is_table = tag.name == 'table'\n",
    "            cellpadding_match = tag.get('cellpadding') in ['1', '7']\n",
    "            border_match = tag.get('border') in ['1', '1px']\n",
    "            frame_match = tag.get('frame') in ['BOX', 'box']\n",
    "            return is_table and cellpadding_match and border_match and frame_match \n",
    "        \n",
    "        #Find all tables that match our custom criteria\n",
    "        tables=soup.find_all(matches_table_attribute)\n",
    "\n",
    "        #Return the first matching table\n",
    "        if tables:\n",
    "            #Parse the table with Pandas\n",
    "            df=pd.read_html(str(tables[0]))[0] #Read the table into a dataframe. The [0] is to get the first table\n",
    "            return df\n",
    "        else:\n",
    "            #Handle the case where no table is found\n",
    "            print(f\"No table found in file: {html_file}\")\n",
    "            return None\n",
    "\n",
    "for file in os.listdir(html_dir): \n",
    "    if file.endswith('.html'):\n",
    "        #Read the aspecific table from the HTML file\n",
    "        df=read_specific_table(os.path.join(html_dir,file))\n",
    "\n",
    "        #Write the DataFrame to a CSV file\n",
    "        csv_file=os.path.splitext(file)[0]+'.csv' #[0] is the file name without the extension\n",
    "        #Save the csv file in the csv directory\n",
    "        df.to_csv(os.path.join(csv_dir,csv_file),index=False)\n",
    "        print(f\"File {csv_file} has been saved.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of columns and filling NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 20030930_Q3_2003_large_commercial_banks.csv\n",
      "Successfully converted 20031231_Q4_2003_large_commercial_banks.csv\n",
      "Successfully converted 20040331_Q1_2004_large_commercial_banks.csv\n",
      "Successfully converted 20040630_Q2_2004_large_commercial_banks.csv\n",
      "Successfully converted 20040930_Q3_2004_large_commercial_banks.csv\n",
      "Successfully converted 20041231_Q4_2004_large_commercial_banks.csv\n",
      "Successfully converted 20050331_Q1_2005_large_commercial_banks.csv\n",
      "Successfully converted 20050630_Q2_2005_large_commercial_banks.csv\n",
      "Successfully converted 20050930_Q3_2005_large_commercial_banks.csv\n",
      "Successfully converted 20051231_Q4_2005_large_commercial_banks.csv\n",
      "Successfully converted 20060331_Q1_2006_large_commercial_banks.csv\n",
      "Successfully converted 20060630_Q2_2006_large_commercial_banks.csv\n",
      "Successfully converted 20060930_Q3_2006_large_commercial_banks.csv\n",
      "Successfully converted 20061231_Q4_2006_large_commercial_banks.csv\n",
      "Successfully converted 20070331_Q1_2007_large_commercial_banks.csv\n",
      "Successfully converted 20070630_Q2_2007_large_commercial_banks.csv\n",
      "Successfully converted 20070930_Q3_2007_large_commercial_banks.csv\n",
      "Successfully converted 20071231_Q4_2007_large_commercial_banks.csv\n",
      "Successfully converted 20080331_Q1_2008_large_commercial_banks.csv\n",
      "Successfully converted 20080630_Q2_2008_large_commercial_banks.csv\n",
      "Successfully converted 20080930_Q3_2008_large_commercial_banks.csv\n",
      "Successfully converted 20081231_Q4_2008_large_commercial_banks.csv\n",
      "Successfully converted 20090331_Q1_2009_large_commercial_banks.csv\n",
      "Successfully converted 20090630_Q2_2009_large_commercial_banks.csv\n",
      "Successfully converted 20090930_Q3_2009_large_commercial_banks.csv\n",
      "Successfully converted 20091231_Q4_2009_large_commercial_banks.csv\n",
      "Successfully converted 20100331_Q1_2010_large_commercial_banks.csv\n",
      "Successfully converted 20100630_Q2_2010_large_commercial_banks.csv\n",
      "Successfully converted 20100930_Q3_2010_large_commercial_banks.csv\n",
      "Successfully converted 20101231_Q4_2010_large_commercial_banks.csv\n",
      "Successfully converted 20110331_Q1_2011_large_commercial_banks.csv\n",
      "Successfully converted 20110630_Q2_2011_large_commercial_banks.csv\n",
      "Successfully converted 20110930_Q3_2011_large_commercial_banks.csv\n",
      "Successfully converted 20111231_Q4_2011_large_commercial_banks.csv\n",
      "Successfully converted 20120331_Q1_2012_large_commercial_banks.csv\n",
      "Successfully converted 20120630_Q2_2012_large_commercial_banks.csv\n",
      "Successfully converted 20120930_Q3_2012_large_commercial_banks.csv\n",
      "Successfully converted 20121231_Q4_2012_large_commercial_banks.csv\n",
      "Successfully converted 20130331_Q1_2013_large_commercial_banks.csv\n",
      "Successfully converted 20130630_Q2_2013_large_commercial_banks.csv\n",
      "Successfully converted 20130930_Q3_2013_large_commercial_banks.csv\n",
      "Successfully converted 20131231_Q4_2013_large_commercial_banks.csv\n",
      "Successfully converted 20140331_Q1_2014_large_commercial_banks.csv\n",
      "Successfully converted 20140630_Q2_2014_large_commercial_banks.csv\n",
      "Successfully converted 20140930_Q3_2014_large_commercial_banks.csv\n",
      "Successfully converted 20141231_Q4_2014_large_commercial_banks.csv\n",
      "Successfully converted 20150331_Q1_2015_large_commercial_banks.csv\n",
      "Successfully converted 20150630_Q2_2015_large_commercial_banks.csv\n",
      "Successfully converted 20150930_Q3_2015_large_commercial_banks.csv\n",
      "Successfully converted 20151231_Q4_2015_large_commercial_banks.csv\n",
      "Successfully converted 20160331_Q1_2016_large_commercial_banks.csv\n",
      "Successfully converted 20160630_Q2_2016_large_commercial_banks.csv\n",
      "Successfully converted 20160930_Q3_2016_large_commercial_banks.csv\n",
      "Successfully converted 20161231_Q4_2016_large_commercial_banks.csv\n",
      "Successfully converted 20170331_Q1_2017_large_commercial_banks.csv\n",
      "Successfully converted 20170630_Q2_2017_large_commercial_banks.csv\n",
      "Successfully converted 20170930_Q3_2017_large_commercial_banks.csv\n",
      "Successfully converted 20171231_Q4_2017_large_commercial_banks.csv\n",
      "Successfully converted 20180331_Q1_2018_large_commercial_banks.csv\n",
      "Successfully converted 20180630_Q2_2018_large_commercial_banks.csv\n",
      "Successfully converted 20180930_Q3_2018_large_commercial_banks.csv\n",
      "Successfully converted 20181231_Q4_2018_large_commercial_banks.csv\n",
      "Successfully converted 20190331_Q1_2019_large_commercial_banks.csv\n",
      "Successfully converted 20190630_Q2_2019_large_commercial_banks.csv\n",
      "Successfully converted 20190930_Q3_2019_large_commercial_banks.csv\n",
      "Successfully converted 20191231_Q4_2019_large_commercial_banks.csv\n",
      "Successfully converted 20200331_Q1_2020_large_commercial_banks.csv\n",
      "Successfully converted 20200630_Q2_2020_large_commercial_banks.csv\n",
      "Successfully converted 20200930_Q3_2020_large_commercial_banks.csv\n",
      "Successfully converted 20201231_Q4_2020_large_commercial_banks.csv\n",
      "Successfully converted 20210331_Q1_2021_large_commercial_banks.csv\n",
      "Successfully converted 20210630_Q2_2021_large_commercial_banks.csv\n",
      "Successfully converted 20210930_Q3_2021_large_commercial_banks.csv\n",
      "Successfully converted 20211231_Q4_2021_large_commercial_banks.csv\n",
      "Successfully converted 20220331_Q1_2022_large_commercial_banks.csv\n",
      "Successfully converted 20220630_Q2_2022_large_commercial_banks.csv\n",
      "Successfully converted 20220930_Q3_2022_large_commercial_banks.csv\n",
      "Successfully converted 20221231_Q4_2022_large_commercial_banks.csv\n",
      "Successfully converted 20230331_Q1_2023_large_commercial_banks.csv\n",
      "Successfully converted 20230630_Q2_2023_large_commercial_banks.csv\n",
      "Successfully converted 20230930_Q3_2023_large_commercial_banks.csv\n",
      "Successfully converted 20231231_Q4_2023_large_commercial_banks.csv\n",
      "Successfully converted 20240331_Q1_2024_large_commercial_banks.csv\n",
      "Successfully converted 20240630_Q2_2024_large_commercial_banks.csv\n",
      "Successfully converted 20240930_Q3_2024_large_commercial_banks.csv\n"
     ]
    }
   ],
   "source": [
    "#Defining the path to the csv files\n",
    "csv_files=os.listdir(csv_dir) #List of all the csv files in the directory\n",
    "\n",
    "#Define the uniform column names\n",
    "uniform_columns= [\n",
    "    'Name',\n",
    "    'Natl Rank',\n",
    "    'Bank ID',\n",
    "    'Bank Location',\n",
    "    'Charter',\n",
    "    'Consolidated Assets',\n",
    "    'Domestic Assets',\n",
    "    'Percentage Domestic Assets',\n",
    "    'Percentage Cumulative Assets',\n",
    "    'Domestic Branches',\n",
    "    'Foreign Branches',\n",
    "    'IBF',\n",
    "    'Percentage Foreign Owned',\n",
    "]\n",
    "\n",
    "#Loop through each CSV file and append to the uniform_columns\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        #construct the full path to the CSV file\n",
    "        csv_path=os.path.join(csv_dir,csv_file)\n",
    "\n",
    "        #Read the CSV file into a Dataframe\n",
    "        df=pd.read_csv(csv_path)\n",
    "\n",
    "        #Check if the DataFrame has the same number of columns as 'uniform_columns'\n",
    "        if len(df.columns) < len(uniform_columns):\n",
    "            #Add the missing columns and fill them with NaN\n",
    "            missing_cols=len(uniform_columns)-len(df.columns)\n",
    "            for _ in range(missing_cols):\n",
    "                df[uniform_columns[-missing_cols]]=np.nan\n",
    "\n",
    "        #Read the columns\n",
    "        df.columns=uniform_columns\n",
    "\n",
    "        #Write the DataFrame back to the CSV file\n",
    "        df.to_csv(csv_path,index=False)\n",
    "\n",
    "        print(f\"Successfully converted {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {csv_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 20030930_Q3_2003_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20030930_Q3_2003_large_commercial_banks.csv\n",
      "Processing file: 20031231_Q4_2003_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20031231_Q4_2003_large_commercial_banks.csv\n",
      "Processing file: 20040331_Q1_2004_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20040331_Q1_2004_large_commercial_banks.csv\n",
      "Processing file: 20040630_Q2_2004_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20040630_Q2_2004_large_commercial_banks.csv\n",
      "Processing file: 20040930_Q3_2004_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20040930_Q3_2004_large_commercial_banks.csv\n",
      "Processing file: 20041231_Q4_2004_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20041231_Q4_2004_large_commercial_banks.csv\n",
      "Processing file: 20050331_Q1_2005_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20050331_Q1_2005_large_commercial_banks.csv\n",
      "Processing file: 20050630_Q2_2005_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20050630_Q2_2005_large_commercial_banks.csv\n",
      "Processing file: 20050930_Q3_2005_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20050930_Q3_2005_large_commercial_banks.csv\n",
      "Processing file: 20051231_Q4_2005_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20051231_Q4_2005_large_commercial_banks.csv\n",
      "Processing file: 20060331_Q1_2006_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20060331_Q1_2006_large_commercial_banks.csv\n",
      "Processing file: 20060630_Q2_2006_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20060630_Q2_2006_large_commercial_banks.csv\n",
      "Processing file: 20060930_Q3_2006_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20060930_Q3_2006_large_commercial_banks.csv\n",
      "Processing file: 20061231_Q4_2006_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20061231_Q4_2006_large_commercial_banks.csv\n",
      "Processing file: 20070331_Q1_2007_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20070331_Q1_2007_large_commercial_banks.csv\n",
      "Processing file: 20070630_Q2_2007_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20070630_Q2_2007_large_commercial_banks.csv\n",
      "Processing file: 20070930_Q3_2007_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20070930_Q3_2007_large_commercial_banks.csv\n",
      "Processing file: 20071231_Q4_2007_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20071231_Q4_2007_large_commercial_banks.csv\n",
      "Processing file: 20080331_Q1_2008_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20080331_Q1_2008_large_commercial_banks.csv\n",
      "Processing file: 20080630_Q2_2008_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20080630_Q2_2008_large_commercial_banks.csv\n",
      "Processing file: 20080930_Q3_2008_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20080930_Q3_2008_large_commercial_banks.csv\n",
      "Processing file: 20081231_Q4_2008_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20081231_Q4_2008_large_commercial_banks.csv\n",
      "Processing file: 20090331_Q1_2009_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20090331_Q1_2009_large_commercial_banks.csv\n",
      "Processing file: 20090630_Q2_2009_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20090630_Q2_2009_large_commercial_banks.csv\n",
      "Processing file: 20090930_Q3_2009_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20090930_Q3_2009_large_commercial_banks.csv\n",
      "Processing file: 20091231_Q4_2009_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20091231_Q4_2009_large_commercial_banks.csv\n",
      "Processing file: 20100331_Q1_2010_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20100331_Q1_2010_large_commercial_banks.csv\n",
      "Processing file: 20100630_Q2_2010_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20100630_Q2_2010_large_commercial_banks.csv\n",
      "Processing file: 20100930_Q3_2010_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20100930_Q3_2010_large_commercial_banks.csv\n",
      "Processing file: 20101231_Q4_2010_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20101231_Q4_2010_large_commercial_banks.csv\n",
      "Processing file: 20110331_Q1_2011_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20110331_Q1_2011_large_commercial_banks.csv\n",
      "Processing file: 20110630_Q2_2011_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20110630_Q2_2011_large_commercial_banks.csv\n",
      "Processing file: 20110930_Q3_2011_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20110930_Q3_2011_large_commercial_banks.csv\n",
      "Processing file: 20111231_Q4_2011_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20111231_Q4_2011_large_commercial_banks.csv\n",
      "Processing file: 20120331_Q1_2012_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20120331_Q1_2012_large_commercial_banks.csv\n",
      "Processing file: 20120630_Q2_2012_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20120630_Q2_2012_large_commercial_banks.csv\n",
      "Processing file: 20120930_Q3_2012_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20120930_Q3_2012_large_commercial_banks.csv\n",
      "Processing file: 20121231_Q4_2012_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20121231_Q4_2012_large_commercial_banks.csv\n",
      "Processing file: 20130331_Q1_2013_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20130331_Q1_2013_large_commercial_banks.csv\n",
      "Processing file: 20130630_Q2_2013_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20130630_Q2_2013_large_commercial_banks.csv\n",
      "Processing file: 20130930_Q3_2013_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20130930_Q3_2013_large_commercial_banks.csv\n",
      "Processing file: 20131231_Q4_2013_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20131231_Q4_2013_large_commercial_banks.csv\n",
      "Processing file: 20140331_Q1_2014_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20140331_Q1_2014_large_commercial_banks.csv\n",
      "Processing file: 20140630_Q2_2014_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20140630_Q2_2014_large_commercial_banks.csv\n",
      "Processing file: 20140930_Q3_2014_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20140930_Q3_2014_large_commercial_banks.csv\n",
      "Processing file: 20141231_Q4_2014_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20141231_Q4_2014_large_commercial_banks.csv\n",
      "Processing file: 20150331_Q1_2015_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20150331_Q1_2015_large_commercial_banks.csv\n",
      "Processing file: 20150630_Q2_2015_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20150630_Q2_2015_large_commercial_banks.csv\n",
      "Processing file: 20150930_Q3_2015_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20150930_Q3_2015_large_commercial_banks.csv\n",
      "Processing file: 20151231_Q4_2015_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20151231_Q4_2015_large_commercial_banks.csv\n",
      "Processing file: 20160331_Q1_2016_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20160331_Q1_2016_large_commercial_banks.csv\n",
      "Processing file: 20160630_Q2_2016_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20160630_Q2_2016_large_commercial_banks.csv\n",
      "Processing file: 20160930_Q3_2016_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20160930_Q3_2016_large_commercial_banks.csv\n",
      "Processing file: 20161231_Q4_2016_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20161231_Q4_2016_large_commercial_banks.csv\n",
      "Processing file: 20170331_Q1_2017_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20170331_Q1_2017_large_commercial_banks.csv\n",
      "Processing file: 20170630_Q2_2017_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20170630_Q2_2017_large_commercial_banks.csv\n",
      "Processing file: 20170930_Q3_2017_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20170930_Q3_2017_large_commercial_banks.csv\n",
      "Processing file: 20171231_Q4_2017_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20171231_Q4_2017_large_commercial_banks.csv\n",
      "Processing file: 20180331_Q1_2018_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20180331_Q1_2018_large_commercial_banks.csv\n",
      "Processing file: 20180630_Q2_2018_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20180630_Q2_2018_large_commercial_banks.csv\n",
      "Processing file: 20180930_Q3_2018_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20180930_Q3_2018_large_commercial_banks.csv\n",
      "Processing file: 20181231_Q4_2018_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20181231_Q4_2018_large_commercial_banks.csv\n",
      "Processing file: 20190331_Q1_2019_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20190331_Q1_2019_large_commercial_banks.csv\n",
      "Processing file: 20190630_Q2_2019_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20190630_Q2_2019_large_commercial_banks.csv\n",
      "Processing file: 20190930_Q3_2019_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20190930_Q3_2019_large_commercial_banks.csv\n",
      "Processing file: 20191231_Q4_2019_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20191231_Q4_2019_large_commercial_banks.csv\n",
      "Processing file: 20200331_Q1_2020_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20200331_Q1_2020_large_commercial_banks.csv\n",
      "Processing file: 20200630_Q2_2020_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20200630_Q2_2020_large_commercial_banks.csv\n",
      "Processing file: 20200930_Q3_2020_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20200930_Q3_2020_large_commercial_banks.csv\n",
      "Processing file: 20201231_Q4_2020_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20201231_Q4_2020_large_commercial_banks.csv\n",
      "Processing file: 20210331_Q1_2021_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20210331_Q1_2021_large_commercial_banks.csv\n",
      "Processing file: 20210630_Q2_2021_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20210630_Q2_2021_large_commercial_banks.csv\n",
      "Processing file: 20210930_Q3_2021_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20210930_Q3_2021_large_commercial_banks.csv\n",
      "Processing file: 20211231_Q4_2021_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20211231_Q4_2021_large_commercial_banks.csv\n",
      "Processing file: 20220331_Q1_2022_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20220331_Q1_2022_large_commercial_banks.csv\n",
      "Processing file: 20220630_Q2_2022_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20220630_Q2_2022_large_commercial_banks.csv\n",
      "Processing file: 20220930_Q3_2022_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20220930_Q3_2022_large_commercial_banks.csv\n",
      "Processing file: 20221231_Q4_2022_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20221231_Q4_2022_large_commercial_banks.csv\n",
      "Processing file: 20230331_Q1_2023_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20230331_Q1_2023_large_commercial_banks.csv\n",
      "Processing file: 20230630_Q2_2023_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20230630_Q2_2023_large_commercial_banks.csv\n",
      "Processing file: 20230930_Q3_2023_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20230930_Q3_2023_large_commercial_banks.csv\n",
      "Processing file: 20231231_Q4_2023_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20231231_Q4_2023_large_commercial_banks.csv\n",
      "Processing file: 20240331_Q1_2024_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20240331_Q1_2024_large_commercial_banks.csv\n",
      "Processing file: 20240630_Q2_2024_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20240630_Q2_2024_large_commercial_banks.csv\n",
      "Processing file: 20240930_Q3_2024_large_commercial_banks.csv\n",
      "Successfully cleaned file: 20240930_Q3_2024_large_commercial_banks.csv\n"
     ]
    }
   ],
   "source": [
    "'''Change datatypes for columns and add new columns'''\n",
    "#Define the columns to covnert the integer\n",
    "integer_columns=['Natl Rank', 'Consolidated Assets', 'Domestic Assets', 'Domestic Branches', 'Foreign Branches']\n",
    "\n",
    "#Define the columns to convert to floats\n",
    "float_columns=['Percentage Domestic Assets', 'Percentage Cumulative Assets', 'Percentage Foreign Owned']\n",
    "\n",
    "#Loop through each CSV file and convert the columns to the correct data types\n",
    "for csv_file in csv_files:\n",
    "    try: #Error handling\n",
    "        print(f\"Processing file: {csv_file}\")\n",
    "        csv_path=os.path.join(csv_dir,csv_file)\n",
    "        df=pd.read_csv(csv_path)\n",
    "\n",
    "        #Convert to float columns\n",
    "        for col in integer_columns:\n",
    "            df[col]=pd.to_numeric(df[col],errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "        #Mutiply 'Consolidated Assets' and 'Domestic Assets' by 1,000,000\n",
    "        df['Consolidated Assets']=df['Consolidated Assets']*1000000 #*1000000 use to multiply the 'Consolidated Assets', 'Domestic Assets' columns\n",
    "        df['Domestic Assets']=df['Domestic Assets']*1000000 #*1000000 use to multiply the 'Consolidated Assets', 'Domestic Assets' columns\n",
    "\n",
    "        #Convert float columns and divide by 100 to get decimal representation\n",
    "        for col in float_columns:\n",
    "            df[col]=pd.to_numeric(df[col],errors='coerce').div(100) #.div(100) use to divide 'Percentage Domestic Assets', 'Percentage Cumulative Assets', 'Percentage Foreign Owned'columns\n",
    "\n",
    "        #Save the cleaned DataFrame back to the CSV file\n",
    "        df.to_csv(csv_path,index=False)\n",
    "\n",
    "        print(f\"Successfully cleaned file: {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {csv_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Columns- Percentage Column\n",
    "Create another column in each one of the csv files that would allow me to see the percentage of the consolidated assets of each bank compared to the sum of the consolidated assets of all banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through all the CSV files to create a percentage column based on the consolidated assets:\n",
    "for csv_file in csv_files:\n",
    "    csv_path=os.path.join(csv_dir,csv_file)\n",
    "    df=pd.read_csv(csv_path)\n",
    "\n",
    "    #Calculate the sum of consolidated assets for each bank\n",
    "    total_assets=df['Consolidated Assets'].sum()\n",
    "\n",
    "    #Calculate the percentage of consolidated assets for each bank\n",
    "    df['Percentage of Total Cosolidated Assets']=(df['Consolidated Assets']/total_assets)\n",
    "\n",
    "    #Round the percentage to 4 decimal places\n",
    "    df['Percentage of Total Cosolidated Assets']=df['Percentage of Total Cosolidated Assets'].round(4)\n",
    "\n",
    "    #Convert 'Percentage of Total Cosolidated Assets' column into a float column\n",
    "    df['Percentage of Total Cosolidated Assets']=df['Percentage of Total Cosolidated Assets'].astype(float)\n",
    "\n",
    "    # Save the updated dataframe back to the CSV file\n",
    "    df.to_csv(csv_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Columns- Date and Quarter\n",
    "Crete date and quarter column in each one of the csv files\n",
    "based on the file name format in which '20170331_Q1_2017_large_commercial_banks.csv' \n",
    "stands for ''yyyymmdd_Qx_yyyy_large_commercial_banks.csv'. \n",
    "\n",
    "The date column should be in the format 'mm/dd/yyyy' (in date data type fomat) and the quarter column \n",
    "should be in the format 'Qx-yyyy' format in date data type fomat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed file: 20030930_Q3_2003_large_commercial_banks.csv\n",
      "Successfully processed file: 20031231_Q4_2003_large_commercial_banks.csv\n",
      "Successfully processed file: 20040331_Q1_2004_large_commercial_banks.csv\n",
      "Successfully processed file: 20040630_Q2_2004_large_commercial_banks.csv\n",
      "Successfully processed file: 20040930_Q3_2004_large_commercial_banks.csv\n",
      "Successfully processed file: 20041231_Q4_2004_large_commercial_banks.csv\n",
      "Successfully processed file: 20050331_Q1_2005_large_commercial_banks.csv\n",
      "Successfully processed file: 20050630_Q2_2005_large_commercial_banks.csv\n",
      "Successfully processed file: 20050930_Q3_2005_large_commercial_banks.csv\n",
      "Successfully processed file: 20051231_Q4_2005_large_commercial_banks.csv\n",
      "Successfully processed file: 20060331_Q1_2006_large_commercial_banks.csv\n",
      "Successfully processed file: 20060630_Q2_2006_large_commercial_banks.csv\n",
      "Successfully processed file: 20060930_Q3_2006_large_commercial_banks.csv\n",
      "Successfully processed file: 20061231_Q4_2006_large_commercial_banks.csv\n",
      "Successfully processed file: 20070331_Q1_2007_large_commercial_banks.csv\n",
      "Successfully processed file: 20070630_Q2_2007_large_commercial_banks.csv\n",
      "Successfully processed file: 20070930_Q3_2007_large_commercial_banks.csv\n",
      "Successfully processed file: 20071231_Q4_2007_large_commercial_banks.csv\n",
      "Successfully processed file: 20080331_Q1_2008_large_commercial_banks.csv\n",
      "Successfully processed file: 20080630_Q2_2008_large_commercial_banks.csv\n",
      "Successfully processed file: 20080930_Q3_2008_large_commercial_banks.csv\n",
      "Successfully processed file: 20081231_Q4_2008_large_commercial_banks.csv\n",
      "Successfully processed file: 20090331_Q1_2009_large_commercial_banks.csv\n",
      "Successfully processed file: 20090630_Q2_2009_large_commercial_banks.csv\n",
      "Successfully processed file: 20090930_Q3_2009_large_commercial_banks.csv\n",
      "Successfully processed file: 20091231_Q4_2009_large_commercial_banks.csv\n",
      "Successfully processed file: 20100331_Q1_2010_large_commercial_banks.csv\n",
      "Successfully processed file: 20100630_Q2_2010_large_commercial_banks.csv\n",
      "Successfully processed file: 20100930_Q3_2010_large_commercial_banks.csv\n",
      "Successfully processed file: 20101231_Q4_2010_large_commercial_banks.csv\n",
      "Successfully processed file: 20110331_Q1_2011_large_commercial_banks.csv\n",
      "Successfully processed file: 20110630_Q2_2011_large_commercial_banks.csv\n",
      "Successfully processed file: 20110930_Q3_2011_large_commercial_banks.csv\n",
      "Successfully processed file: 20111231_Q4_2011_large_commercial_banks.csv\n",
      "Successfully processed file: 20120331_Q1_2012_large_commercial_banks.csv\n",
      "Successfully processed file: 20120630_Q2_2012_large_commercial_banks.csv\n",
      "Successfully processed file: 20120930_Q3_2012_large_commercial_banks.csv\n",
      "Successfully processed file: 20121231_Q4_2012_large_commercial_banks.csv\n",
      "Successfully processed file: 20130331_Q1_2013_large_commercial_banks.csv\n",
      "Successfully processed file: 20130630_Q2_2013_large_commercial_banks.csv\n",
      "Successfully processed file: 20130930_Q3_2013_large_commercial_banks.csv\n",
      "Successfully processed file: 20131231_Q4_2013_large_commercial_banks.csv\n",
      "Successfully processed file: 20140331_Q1_2014_large_commercial_banks.csv\n",
      "Successfully processed file: 20140630_Q2_2014_large_commercial_banks.csv\n",
      "Successfully processed file: 20140930_Q3_2014_large_commercial_banks.csv\n",
      "Successfully processed file: 20141231_Q4_2014_large_commercial_banks.csv\n",
      "Successfully processed file: 20150331_Q1_2015_large_commercial_banks.csv\n",
      "Successfully processed file: 20150630_Q2_2015_large_commercial_banks.csv\n",
      "Successfully processed file: 20150930_Q3_2015_large_commercial_banks.csv\n",
      "Successfully processed file: 20151231_Q4_2015_large_commercial_banks.csv\n",
      "Successfully processed file: 20160331_Q1_2016_large_commercial_banks.csv\n",
      "Successfully processed file: 20160630_Q2_2016_large_commercial_banks.csv\n",
      "Successfully processed file: 20160930_Q3_2016_large_commercial_banks.csv\n",
      "Successfully processed file: 20161231_Q4_2016_large_commercial_banks.csv\n",
      "Successfully processed file: 20170331_Q1_2017_large_commercial_banks.csv\n",
      "Successfully processed file: 20170630_Q2_2017_large_commercial_banks.csv\n",
      "Successfully processed file: 20170930_Q3_2017_large_commercial_banks.csv\n",
      "Successfully processed file: 20171231_Q4_2017_large_commercial_banks.csv\n",
      "Successfully processed file: 20180331_Q1_2018_large_commercial_banks.csv\n",
      "Successfully processed file: 20180630_Q2_2018_large_commercial_banks.csv\n",
      "Successfully processed file: 20180930_Q3_2018_large_commercial_banks.csv\n",
      "Successfully processed file: 20181231_Q4_2018_large_commercial_banks.csv\n",
      "Successfully processed file: 20190331_Q1_2019_large_commercial_banks.csv\n",
      "Successfully processed file: 20190630_Q2_2019_large_commercial_banks.csv\n",
      "Successfully processed file: 20190930_Q3_2019_large_commercial_banks.csv\n",
      "Successfully processed file: 20191231_Q4_2019_large_commercial_banks.csv\n",
      "Successfully processed file: 20200331_Q1_2020_large_commercial_banks.csv\n",
      "Successfully processed file: 20200630_Q2_2020_large_commercial_banks.csv\n",
      "Successfully processed file: 20200930_Q3_2020_large_commercial_banks.csv\n",
      "Successfully processed file: 20201231_Q4_2020_large_commercial_banks.csv\n",
      "Successfully processed file: 20210331_Q1_2021_large_commercial_banks.csv\n",
      "Successfully processed file: 20210630_Q2_2021_large_commercial_banks.csv\n",
      "Successfully processed file: 20210930_Q3_2021_large_commercial_banks.csv\n",
      "Successfully processed file: 20211231_Q4_2021_large_commercial_banks.csv\n",
      "Successfully processed file: 20220331_Q1_2022_large_commercial_banks.csv\n",
      "Successfully processed file: 20220630_Q2_2022_large_commercial_banks.csv\n",
      "Successfully processed file: 20220930_Q3_2022_large_commercial_banks.csv\n",
      "Successfully processed file: 20221231_Q4_2022_large_commercial_banks.csv\n",
      "Successfully processed file: 20230331_Q1_2023_large_commercial_banks.csv\n",
      "Successfully processed file: 20230630_Q2_2023_large_commercial_banks.csv\n",
      "Successfully processed file: 20230930_Q3_2023_large_commercial_banks.csv\n",
      "Successfully processed file: 20231231_Q4_2023_large_commercial_banks.csv\n",
      "Successfully processed file: 20240331_Q1_2024_large_commercial_banks.csv\n",
      "Successfully processed file: 20240630_Q2_2024_large_commercial_banks.csv\n",
      "Successfully processed file: 20240930_Q3_2024_large_commercial_banks.csv\n"
     ]
    }
   ],
   "source": [
    "#Loop through all the CSV files to create the date and quarter columns\n",
    "for csv_file in csv_files:\n",
    "    csv_path=os.path.join(csv_dir,csv_file)\n",
    "    df=pd.read_csv(csv_path)\n",
    "\n",
    "    #Extract the date from the file name\n",
    "    date_str=csv_file.split('_')[0] #split('_')[0] use to extract the date from the file name. The [0] is the first element in the list\n",
    "    date=pd.to_datetime(date_str,format='%Y%m%d')\n",
    "    df['Date']=date\n",
    "\n",
    "    #Extract the quarter from the file name\n",
    "    quarter_str=csv_file.split('_')[1] #split('_')[1] use to extract the quarter from the file name. The [1] is the second element in the list\n",
    "    year_str=csv_file.split('_')[2] #split('_')[2] use to extract the year from the file name. The [2] is the third element in the list\n",
    "    quarter=f\"{quarter_str}-{year_str}\" #f\"{quarter_str}-{year_str}\" use to format the quarter and year\n",
    "    df['Quarter']=quarter #Add the quarter column to the DataFrame\n",
    "\n",
    "    #Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(csv_path,index=False)\n",
    "\n",
    "    print(f\"Successfully processed file: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path to save the plot ready dataframes in the form of csv files\n",
    "dataset_dir='./datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master dataset\n",
    "Function reads multiple CSV files and concatenates them into a single DataFrame.\n",
    " * It sorts the DataFrame by 'Date' in the descending order.\n",
    " * It groups the DataFrame by 'Bank ID' and gets the first 'Name' for each group.\n",
    " * It maps the bank_names_series to the 'Bank ID' column in the concatenated_df.\n",
    " * It sums the consolidated assets of all banks per quarter.\n",
    " * It turns 'total_assets_per_quarter' into a dataframe.\n",
    " * It saves the concatenated_df dataframe to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(csv_files, csv_dir):\n",
    "    # Create an empty list to store the dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over each CSV file\n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a dataframe\n",
    "        df = pd.read_csv(os.path.join(csv_dir, file))\n",
    "        # Append the dataframe to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all dataframes into a single dataframe\n",
    "    concatenated_df = pd.concat(dfs)\n",
    "\n",
    "    # Sort the dataframe by 'Date' in the descending order\n",
    "    concatenated_df = concatenated_df.sort_values('Date', ascending=False)\n",
    "\n",
    "    # Group by 'Bank ID' and get the first 'Name' for each group\n",
    "    bank_names_series = concatenated_df.groupby(by='Bank ID')['Name'].first()\n",
    "\n",
    "    # Map the bank_names_series to the 'Bank ID' column in the concatenated_df\n",
    "    concatenated_df['Bank Name'] = concatenated_df['Bank ID'].map(bank_names_series)\n",
    "\n",
    "    # Sum the consolidated assets of all banks per quarter\n",
    "    total_assets_per_quarter = concatenated_df.groupby('Quarter')['Consolidated Assets'].sum()\n",
    "\n",
    "    # Turn 'total_assets_per_quarter' into a dataframe\n",
    "    total_assets_per_quarter = total_assets_per_quarter.to_frame()\n",
    "\n",
    "    #Save the concatenated_df dataframe to a CSV file\n",
    "    concatenated_df.to_csv(os.path.join(dataset_dir, 'concatenated_df.csv'), index=False)\n",
    "\n",
    "    return concatenated_df, total_assets_per_quarter\n",
    "\n",
    "\n",
    "    # Call the process_data function\n",
    "concatenated_df, total_assets_per_quarter = process_data(csv_files, csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consolidated Assets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quarter</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q1-2004</th>\n",
       "      <td>6.982157e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1-2005</th>\n",
       "      <td>7.739072e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1-2006</th>\n",
       "      <td>8.473557e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1-2007</th>\n",
       "      <td>9.222430e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1-2008</th>\n",
       "      <td>1.053344e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4-2019</th>\n",
       "      <td>1.691051e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4-2020</th>\n",
       "      <td>1.991083e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4-2021</th>\n",
       "      <td>2.157899e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4-2022</th>\n",
       "      <td>2.169659e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4-2023</th>\n",
       "      <td>2.184642e+13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Consolidated Assets\n",
       "Quarter                     \n",
       "Q1-2004         6.982157e+12\n",
       "Q1-2005         7.739072e+12\n",
       "Q1-2006         8.473557e+12\n",
       "Q1-2007         9.222430e+12\n",
       "Q1-2008         1.053344e+13\n",
       "...                      ...\n",
       "Q4-2019         1.691051e+13\n",
       "Q4-2020         1.991083e+13\n",
       "Q4-2021         2.157899e+13\n",
       "Q4-2022         2.169659e+13\n",
       "Q4-2023         2.184642e+13\n",
       "\n",
       "[85 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_assets_per_quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Dataframe for the line plot\n",
    "\n",
    "Function creates a pivot table of the concatenated_df dataframe to set 'Date' as the index, 'Commercial Name' as the columns, and 'Total Assets' as the values.\n",
    " * It adds 'Total Assets' column to the big_four_pivot dataframe.\n",
    " * It sums the consolidated assets of the Big Four banks for each quarter and adds the result to the 'big_four_pivot' dataframe as 'Big Four Assets'.\n",
    " * It adds 'Other Banks' column to the 'big_four_pivot' dataframe by subtracting 'Total Assets' from 'Big Four Assets'.\n",
    " * It adds 'Share of Consolidated Assets' column to the 'big_four_pivot' dataframe by dividing 'Big Four Assets' by 'Total Assets'.\n",
    " * It saves the 'big_four_pivot' dataframe to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16193\\AppData\\Local\\Temp\\ipykernel_24304\\364427065.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  big_four_df['Commercial Name'] = big_four_df['Bank ID'].map(big_four_commercial_names)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Commercial Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Bank of America</th>\n",
       "      <th>Chase</th>\n",
       "      <th>Citibank</th>\n",
       "      <th>Wells Fargo</th>\n",
       "      <th>Total Assets</th>\n",
       "      <th>Big Four Assets</th>\n",
       "      <th>Share of Consolidated Assets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-09-30</td>\n",
       "      <td>Q3-2003</td>\n",
       "      <td>6.247230e+11</td>\n",
       "      <td>6.381200e+11</td>\n",
       "      <td>5.545400e+11</td>\n",
       "      <td>2.243760e+11</td>\n",
       "      <td>6.664665e+12</td>\n",
       "      <td>2.041759e+12</td>\n",
       "      <td>30.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>Q4-2003</td>\n",
       "      <td>6.179620e+11</td>\n",
       "      <td>6.286620e+11</td>\n",
       "      <td>5.821230e+11</td>\n",
       "      <td>2.504740e+11</td>\n",
       "      <td>6.773933e+12</td>\n",
       "      <td>2.079221e+12</td>\n",
       "      <td>30.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-03-31</td>\n",
       "      <td>Q1-2004</td>\n",
       "      <td>6.905730e+11</td>\n",
       "      <td>6.486920e+11</td>\n",
       "      <td>6.061910e+11</td>\n",
       "      <td>3.475600e+11</td>\n",
       "      <td>6.982157e+12</td>\n",
       "      <td>2.293016e+12</td>\n",
       "      <td>32.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-06-30</td>\n",
       "      <td>Q2-2004</td>\n",
       "      <td>7.068880e+11</td>\n",
       "      <td>6.546410e+11</td>\n",
       "      <td>6.482430e+11</td>\n",
       "      <td>3.646980e+11</td>\n",
       "      <td>7.222725e+12</td>\n",
       "      <td>2.374470e+12</td>\n",
       "      <td>32.87%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>Q3-2004</td>\n",
       "      <td>7.406950e+11</td>\n",
       "      <td>6.617720e+11</td>\n",
       "      <td>6.513450e+11</td>\n",
       "      <td>3.629730e+11</td>\n",
       "      <td>7.411606e+12</td>\n",
       "      <td>2.416785e+12</td>\n",
       "      <td>32.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>Q3-2023</td>\n",
       "      <td>2.465234e+12</td>\n",
       "      <td>3.385581e+12</td>\n",
       "      <td>1.657372e+12</td>\n",
       "      <td>1.704891e+12</td>\n",
       "      <td>2.159586e+13</td>\n",
       "      <td>9.213078e+12</td>\n",
       "      <td>42.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>Q4-2023</td>\n",
       "      <td>2.540116e+12</td>\n",
       "      <td>3.395126e+12</td>\n",
       "      <td>1.684710e+12</td>\n",
       "      <td>1.733244e+12</td>\n",
       "      <td>2.184642e+13</td>\n",
       "      <td>9.353196e+12</td>\n",
       "      <td>42.81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>Q1-2024</td>\n",
       "      <td>2.550363e+12</td>\n",
       "      <td>3.503360e+12</td>\n",
       "      <td>1.698856e+12</td>\n",
       "      <td>1.743283e+12</td>\n",
       "      <td>2.216126e+13</td>\n",
       "      <td>9.495862e+12</td>\n",
       "      <td>42.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>Q2-2024</td>\n",
       "      <td>2.550584e+12</td>\n",
       "      <td>3.510536e+12</td>\n",
       "      <td>1.678936e+12</td>\n",
       "      <td>1.719839e+12</td>\n",
       "      <td>2.209674e+13</td>\n",
       "      <td>9.459895e+12</td>\n",
       "      <td>42.81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>Q3-2024</td>\n",
       "      <td>2.565878e+12</td>\n",
       "      <td>3.584105e+12</td>\n",
       "      <td>1.733111e+12</td>\n",
       "      <td>1.698675e+12</td>\n",
       "      <td>2.241638e+13</td>\n",
       "      <td>9.581769e+12</td>\n",
       "      <td>42.74%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Commercial Name        Date  Quarter  Bank of America         Chase  \\\n",
       "0                2003-09-30  Q3-2003     6.247230e+11  6.381200e+11   \n",
       "1                2003-12-31  Q4-2003     6.179620e+11  6.286620e+11   \n",
       "2                2004-03-31  Q1-2004     6.905730e+11  6.486920e+11   \n",
       "3                2004-06-30  Q2-2004     7.068880e+11  6.546410e+11   \n",
       "4                2004-09-30  Q3-2004     7.406950e+11  6.617720e+11   \n",
       "..                      ...      ...              ...           ...   \n",
       "80               2023-09-30  Q3-2023     2.465234e+12  3.385581e+12   \n",
       "81               2023-12-31  Q4-2023     2.540116e+12  3.395126e+12   \n",
       "82               2024-03-31  Q1-2024     2.550363e+12  3.503360e+12   \n",
       "83               2024-06-30  Q2-2024     2.550584e+12  3.510536e+12   \n",
       "84               2024-09-30  Q3-2024     2.565878e+12  3.584105e+12   \n",
       "\n",
       "Commercial Name      Citibank   Wells Fargo  Total Assets  Big Four Assets  \\\n",
       "0                5.545400e+11  2.243760e+11  6.664665e+12     2.041759e+12   \n",
       "1                5.821230e+11  2.504740e+11  6.773933e+12     2.079221e+12   \n",
       "2                6.061910e+11  3.475600e+11  6.982157e+12     2.293016e+12   \n",
       "3                6.482430e+11  3.646980e+11  7.222725e+12     2.374470e+12   \n",
       "4                6.513450e+11  3.629730e+11  7.411606e+12     2.416785e+12   \n",
       "..                        ...           ...           ...              ...   \n",
       "80               1.657372e+12  1.704891e+12  2.159586e+13     9.213078e+12   \n",
       "81               1.684710e+12  1.733244e+12  2.184642e+13     9.353196e+12   \n",
       "82               1.698856e+12  1.743283e+12  2.216126e+13     9.495862e+12   \n",
       "83               1.678936e+12  1.719839e+12  2.209674e+13     9.459895e+12   \n",
       "84               1.733111e+12  1.698675e+12  2.241638e+13     9.581769e+12   \n",
       "\n",
       "Commercial Name Share of Consolidated Assets  \n",
       "0                                     30.64%  \n",
       "1                                     30.69%  \n",
       "2                                     32.84%  \n",
       "3                                     32.87%  \n",
       "4                                     32.61%  \n",
       "..                                       ...  \n",
       "80                                    42.66%  \n",
       "81                                    42.81%  \n",
       "82                                    42.85%  \n",
       "83                                    42.81%  \n",
       "84                                    42.74%  \n",
       "\n",
       "[85 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_big_four_line(concatenated_df, total_assets_per_quarter):\n",
    "    big_four_banks = {\n",
    "        'JPMORGAN CHASE BK NA/JPMORGAN CHASE & CO': 852218,\n",
    "        'BANK OF AMER NA/BANK OF AMER CORP': 480228,\n",
    "        'CITIBANK NA/CITIGROUP': 476810,\n",
    "        'WELLS FARGO BK NA/WELLS FARGO & CO': 451965\n",
    "    }\n",
    "\n",
    "    # Filter the dataframe to only include the Big Four banks\n",
    "    big_four_df = concatenated_df[concatenated_df['Bank ID'].isin(big_four_banks.values())]\n",
    "\n",
    "    # Create a dictionary to map the Big Four banks to their commercial names\n",
    "    big_four_commercial_names = {\n",
    "        852218: 'Chase',\n",
    "        480228: 'Bank of America',\n",
    "        476810: 'Citibank',\n",
    "        451965: 'Wells Fargo'\n",
    "    }\n",
    "\n",
    "    # Add a 'Commercial Name' column to the big_four_df dataframe\n",
    "    big_four_df['Commercial Name'] = big_four_df['Bank ID'].map(big_four_commercial_names)\n",
    "\n",
    "    # Pivot the big_four_df dataframe to set 'Date' as the index, 'Commercial Name' as the columns, and 'Total Assets' as the values\n",
    "    big_four_pivot = big_four_df.pivot_table(index=['Date', 'Quarter'], columns='Commercial Name', values='Consolidated Assets').reset_index()\n",
    "\n",
    "    # Add 'Total Assets' column to the big_four_pivot dataframe\n",
    "    big_four_pivot['Total Assets'] = big_four_pivot['Quarter'].map(total_assets_per_quarter['Consolidated Assets'])\n",
    "\n",
    "    # Sum the consolidated assets of the Big Four banks for each quarter and add the result to the 'big_four_pivot' dataframe as 'Big Four Assets'\n",
    "    big_four_pivot['Big Four Assets'] = big_four_pivot['Chase'] + big_four_pivot['Bank of America'] + big_four_pivot['Citibank'] + big_four_pivot['Wells Fargo']\n",
    "\n",
    "    # Divide the 'Total Assets' column by the 'Big Four Assets' column to get the percentage of the total assets held by the Big Four banks and add the result to the 'big_four_pivot' dataframe as 'Percentage of Total Assets'\n",
    "    big_four_pivot['Share of Consolidated Assets'] = big_four_pivot['Big Four Assets'] / big_four_pivot['Total Assets']\n",
    "\n",
    "    # Display the 'Share of Consolidated Assets' column in percentage format with two decimal places\n",
    "    big_four_pivot['Share of Consolidated Assets'] = big_four_pivot['Share of Consolidated Assets'].map(lambda x: ' {:.2%}'.format(x))\n",
    "\n",
    "    # Save the big_four_pivot DataFrame to a CSV file\n",
    "    big_four_pivot.to_csv(os.path.join(dataset_dir, 'bank_asset_line.csv'), index=False)\n",
    "\n",
    "    #big_four_pivot.to_csv(os.path.join(csv_dir, 'line_plot.csv'), index=False)\n",
    "\n",
    "    return big_four_pivot\n",
    "\n",
    "# Call the create_big_four_line function\n",
    "\n",
    "big_four_pivot = create_big_four_line(concatenated_df, total_assets_per_quarter)\n",
    "big_four_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the dataframe for the scatter plot\n",
    "\n",
    " * The function 'create_melted_scatter_df' melts the 'big_four_pivot' dataframe to create a new dataframe called 'melted_line_df'.\n",
    " * The 'melted_line_df' dataframe is then saved to a CSV file in the 'dataset_dir' directory.\n",
    " * It also defines a function 'quarter_to_num' to convert the 'Quarter' column to a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Date</th>\n",
       "      <th>Bank</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Numeric_Quarter</th>\n",
       "      <th>Quarter_Ordinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q3-2003</td>\n",
       "      <td>2003-09-30</td>\n",
       "      <td>Chase</td>\n",
       "      <td>6.381200e+11</td>\n",
       "      <td>2003.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q4-2003</td>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>Chase</td>\n",
       "      <td>6.286620e+11</td>\n",
       "      <td>2004.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1-2004</td>\n",
       "      <td>2004-03-31</td>\n",
       "      <td>Chase</td>\n",
       "      <td>6.486920e+11</td>\n",
       "      <td>2004.25</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q2-2004</td>\n",
       "      <td>2004-06-30</td>\n",
       "      <td>Chase</td>\n",
       "      <td>6.546410e+11</td>\n",
       "      <td>2004.50</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q3-2004</td>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>Chase</td>\n",
       "      <td>6.617720e+11</td>\n",
       "      <td>2004.75</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>Q3-2023</td>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>1.704891e+12</td>\n",
       "      <td>2023.75</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>Q4-2023</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>1.733244e+12</td>\n",
       "      <td>2024.00</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Q1-2024</td>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>1.743283e+12</td>\n",
       "      <td>2024.25</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Q2-2024</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>1.719839e+12</td>\n",
       "      <td>2024.50</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Q3-2024</td>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>1.698675e+12</td>\n",
       "      <td>2024.75</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Quarter        Date         Bank        Assets  Numeric_Quarter  \\\n",
       "0    Q3-2003  2003-09-30        Chase  6.381200e+11          2003.75   \n",
       "1    Q4-2003  2003-12-31        Chase  6.286620e+11          2004.00   \n",
       "2    Q1-2004  2004-03-31        Chase  6.486920e+11          2004.25   \n",
       "3    Q2-2004  2004-06-30        Chase  6.546410e+11          2004.50   \n",
       "4    Q3-2004  2004-09-30        Chase  6.617720e+11          2004.75   \n",
       "..       ...         ...          ...           ...              ...   \n",
       "335  Q3-2023  2023-09-30  Wells Fargo  1.704891e+12          2023.75   \n",
       "336  Q4-2023  2023-12-31  Wells Fargo  1.733244e+12          2024.00   \n",
       "337  Q1-2024  2024-03-31  Wells Fargo  1.743283e+12          2024.25   \n",
       "338  Q2-2024  2024-06-30  Wells Fargo  1.719839e+12          2024.50   \n",
       "339  Q3-2024  2024-09-30  Wells Fargo  1.698675e+12          2024.75   \n",
       "\n",
       "     Quarter_Ordinal  \n",
       "0                  1  \n",
       "1                  2  \n",
       "2                  3  \n",
       "3                  4  \n",
       "4                  5  \n",
       "..               ...  \n",
       "335               81  \n",
       "336               82  \n",
       "337               83  \n",
       "338               84  \n",
       "339               85  \n",
       "\n",
       "[340 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_melted_scatter_df(big_four_pivot, dataset_dir):\n",
    "    # Melt the DataFrame\n",
    "    melted_line_df = big_four_pivot.melt(id_vars=['Quarter','Date'], value_vars=['Chase', 'Bank of America', 'Citibank', 'Wells Fargo'], var_name='Bank', value_name='Assets')\n",
    "\n",
    "    # Define function to convert the quarter string to a numerical format\n",
    "    def quarter_to_num(quarter_string):\n",
    "        # Split the quarter string into year and quarter parts\n",
    "        parts = quarter_string.split('-')\n",
    "        year = int(parts[1])\n",
    "        quarter = parts[0]\n",
    "\n",
    "        # Map the quarter part to a fraction of the year\n",
    "        quarter_mapping = {'q1': 0.25, 'q2': 0.5, 'q3': 0.75, 'q4': 1.0}\n",
    "        numerical_quarter = year + quarter_mapping[quarter.lower()]\n",
    "\n",
    "        return numerical_quarter\n",
    "\n",
    "    # Apply the quarter_to_num function to the 'Quarter' column\n",
    "    melted_line_df['Numeric_Quarter'] = melted_line_df['Quarter'].apply(quarter_to_num)\n",
    "\n",
    "    # Create the 'Quarter_Ordinal' column\n",
    "    melted_line_df['Quarter_Ordinal'] = melted_line_df['Numeric_Quarter'].rank(method='dense').astype(int)\n",
    "\n",
    "    # Save melted_line_df to a CSV file to the dataset_dir\n",
    "    melted_line_df.to_csv(os.path.join(dataset_dir, 'bank_asset_scatter.csv'))\n",
    "\n",
    "\n",
    "    return melted_line_df\n",
    "\n",
    "# Call the create_melted_scatter_df function\n",
    "create_melted_scatter_df(big_four_pivot, dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the dataframe for the racing pie chart\n",
    "\n",
    "'transform_and_save' function transforms the 'big_four_pivot' dataframe to calculate the percentage share of each bank's assets and the percentage of the total assets held by the rest of the other banks.\n",
    "It then saves the 'percentage_df' dataframe to a Excel file in the 'dataset_dir' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Bank of America</th>\n",
       "      <th>Chase</th>\n",
       "      <th>Citibank</th>\n",
       "      <th>Wells Fargo</th>\n",
       "      <th>Other Banks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-09-30</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0957</td>\n",
       "      <td>0.0832</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.6936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>0.0912</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.0859</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-03-31</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>0.6716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-06-30</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>0.6713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>0.0999</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>0.0879</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.6739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>0.1568</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.5734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0.1163</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>0.1151</td>\n",
       "      <td>0.1581</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.5715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.1589</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0778</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.1599</td>\n",
       "      <td>0.0773</td>\n",
       "      <td>0.0758</td>\n",
       "      <td>0.5726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Bank of America   Chase  Citibank  Wells Fargo  Other Banks\n",
       "0  2003-09-30           0.0937  0.0957    0.0832       0.0337       0.6936\n",
       "1  2003-12-31           0.0912  0.0928    0.0859       0.0370       0.6931\n",
       "2  2004-03-31           0.0989  0.0929    0.0868       0.0498       0.6716\n",
       "3  2004-06-30           0.0979  0.0906    0.0898       0.0505       0.6713\n",
       "4  2004-09-30           0.0999  0.0893    0.0879       0.0490       0.6739\n",
       "..        ...              ...     ...       ...          ...          ...\n",
       "80 2023-09-30           0.1142  0.1568    0.0767       0.0789       0.5734\n",
       "81 2023-12-31           0.1163  0.1554    0.0771       0.0793       0.5719\n",
       "82 2024-03-31           0.1151  0.1581    0.0767       0.0787       0.5715\n",
       "83 2024-06-30           0.1154  0.1589    0.0760       0.0778       0.5719\n",
       "84 2024-09-30           0.1145  0.1599    0.0773       0.0758       0.5726\n",
       "\n",
       "[85 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_and_save(big_four_pivot, dataset_dir):\n",
    "    # Create a copy of the original dataframe\n",
    "    percentage_df = big_four_pivot.copy()\n",
    "\n",
    "    columns_to_convert = ['Bank of America', 'Chase', 'Citibank', 'Wells Fargo']\n",
    "\n",
    "    # List of columns to drop\n",
    "    columns_to_drop = ['Quarter', 'Total Assets', 'Big Four Assets', 'Share of Consolidated Assets']\n",
    "\n",
    "    for column in columns_to_convert:\n",
    "        # Calculate the percentage share of each bank's assets\n",
    "        percentage_df[column] = percentage_df[column] / percentage_df['Total Assets']\n",
    "\n",
    "        # Add a column that calculates the percentage of the total assets of the rest of the other banks\n",
    "        percentage_df['Other Banks'] = (percentage_df['Total Assets'] - percentage_df['Big Four Assets']) / percentage_df['Total Assets']\n",
    "\n",
    "        #Reduce the decimal places to two and maintain the datatype as a float\n",
    "        percentage_df[column] = percentage_df[column].map(lambda x: round(x, 4))\n",
    "\n",
    "        percentage_df['Other Banks'] = percentage_df['Other Banks'].map(lambda x: round(x, 4))\n",
    "\n",
    "        \n",
    "    # Remove the name of the index\n",
    "    percentage_df.columns.name = None\n",
    "\n",
    "    #Set the 'date' column as datetime datatype\n",
    "    percentage_df['Date'] = pd.to_datetime(percentage_df['Date'])\n",
    "\n",
    "    # Reset the index without keeping old index\n",
    "    percentage_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Drop the columns in the 'columns_to_drop' list\n",
    "    percentage_df = percentage_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Save the percentage_df dataframe to a Excel file\n",
    "    percentage_df.to_excel(os.path.join(dataset_dir, 'bank_asset_percentage.xlsx'), index=False)\n",
    "\n",
    "    return percentage_df\n",
    "\n",
    "percentage_df=transform_and_save(big_four_pivot, dataset_dir)\n",
    "percentage_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the dataframe for the treemap\n",
    "The function 'create_treemap_df' creates a new dataframe called 'treemap_df' from the 'percentage_df' dataframe.\n",
    "It then saves the 'treemap_df' dataframe to a CSV file in the 'dataset_dir' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parent</th>\n",
       "      <th>Bank</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>Consolidated Assets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big Four</td>\n",
       "      <td>Bank of America</td>\n",
       "      <td>11.45%</td>\n",
       "      <td>2565878000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big Four</td>\n",
       "      <td>Chase</td>\n",
       "      <td>15.99%</td>\n",
       "      <td>3584105000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big Four</td>\n",
       "      <td>Citibank</td>\n",
       "      <td>7.73%</td>\n",
       "      <td>1733111000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Four</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>7.58%</td>\n",
       "      <td>1698675000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Other Banks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.26%</td>\n",
       "      <td>12834615000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Parent             Bank Percentage Consolidated Assets\n",
       "1     Big Four  Bank of America     11.45%     2565878000000.0\n",
       "2     Big Four            Chase     15.99%     3584105000000.0\n",
       "3     Big Four         Citibank      7.73%     1733111000000.0\n",
       "4     Big Four      Wells Fargo      7.58%     1698675000000.0\n",
       "5  Other Banks              NaN     57.26%    12834615000000.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_treemap_df(percentage_df, big_four_pivot, dataset_dir):\n",
    "    # Create a copy of the original dataframe\n",
    "    assets_df = big_four_pivot.copy()\n",
    "\n",
    "    columns_of_interest = ['Bank of America', 'Chase', 'Citibank', 'Wells Fargo']\n",
    "\n",
    "    # List of columns to drop\n",
    "    columns_to_drop = ['Quarter', 'Share of Consolidated Assets']\n",
    "\n",
    "    # Remove the name of the index\n",
    "    assets_df.columns.name = None\n",
    "\n",
    "    assets_df['Date'] = pd.to_datetime(assets_df['Date'])\n",
    "\n",
    "    # Drop the columns in the 'columns_to_drop' list\n",
    "    assets_df = assets_df.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Subtract 'Big Four Assets' from 'Total Assets' to get the 'Other Banks' column\n",
    "    assets_df['Other Banks'] = assets_df['Total Assets'] - assets_df['Big Four Assets']\n",
    "\n",
    "    # Convert the 'Date' column to datetime\n",
    "    last_date_index = percentage_df['Date'].idxmax()\n",
    "\n",
    "    # Select the row with the latest date\n",
    "    latest_date = percentage_df.loc[last_date_index]\n",
    "\n",
    "    # Convert the series into a dataframe and transpose it\n",
    "    treemap_df = latest_date.to_frame()\n",
    "\n",
    "    # Reset the index for treemap_df\n",
    "    if not treemap_df.index.equals(pd.RangeIndex(start=0, stop=len(treemap_df))):\n",
    "        treemap_df = treemap_df.reset_index()\n",
    "\n",
    "    # Name the columns of the treemap_df dataframe\n",
    "    treemap_df = treemap_df.rename(columns={treemap_df.columns[0]: 'Bank', treemap_df.columns[1]: 'Percentage'})\n",
    "\n",
    "    # Save the 'Date' value from the first row as a variable\n",
    "    date_note = treemap_df.loc[0, 'Percentage']\n",
    "\n",
    "    # Drop the first row of the dataframe\n",
    "    treemap_df = treemap_df.drop(0)\n",
    "\n",
    "    # Convert the Timestamp into a string and slice it to only include the date part\n",
    "    date_note = str(date_note)[:10]\n",
    "\n",
    "    # In the 'date_note variable, re-arrange string to place month first, then day, and finally year as in 'mm-dd-yyyy' format\n",
    "    date_note = date_note[5:7] + '-' + date_note[8:10] + '-' + date_note[:4]\n",
    "\n",
    "    # Set the name of the Dataframe to the 'date_note'\n",
    "    treemap_df.name = date_note\n",
    "\n",
    "    # Convert the 'Percentage' column into % format\n",
    "    treemap_df['Percentage'] = treemap_df['Percentage'].apply(lambda x: ' {:.2%}'.format(x) if pd.notnull(x) and isinstance(x, (int, float)) else x)\n",
    "\n",
    "    # Create a new 'Parent' column and place it as the first column in the dataframe\n",
    "    treemap_df['Parent'] = 'Big Four'\n",
    "\n",
    "    # For the 5th row in the dataframe, I need to replace the 'Parent' value with 'Other Banks'\n",
    "    treemap_df.loc[5, 'Parent'] = 'Other Banks'\n",
    "    # leave the 'Bank' value as NaN\n",
    "    treemap_df.loc[5, 'Bank'] = np.nan\n",
    "\n",
    "    # Convert the 'Date' column to datetime\n",
    "    last_date_assets = assets_df['Date'].idxmax()\n",
    "\n",
    "    # Select the row with the latest date\n",
    "    last_date_assets = assets_df.loc[last_date_index]\n",
    "\n",
    "    # Drop first row of the dataframe\n",
    "    last_date_assets = last_date_assets.drop('Date')\n",
    "\n",
    "    # Drop the 'Total Assets' column\n",
    "    last_date_assets = last_date_assets.drop('Total Assets')\n",
    "\n",
    "    # Drop the 'Big Four Assets' column\n",
    "    last_date_assets = last_date_assets.drop('Big Four Assets')\n",
    "\n",
    "    # Reset the index for last_date_assets\n",
    "    last_date_assets = last_date_assets.reset_index()\n",
    "\n",
    "    # Name the first column 'Banks'\n",
    "    last_date_assets = last_date_assets.rename(columns={last_date_assets.columns[0]: 'Bank'})\n",
    "\n",
    "    # Rename the second columns as 'Assets'\n",
    "    last_date_assets = last_date_assets.rename(columns={last_date_assets.columns[1]: 'Consolidated Assets'})\n",
    "\n",
    "    # Create a variable named 'Total Assets' that has the value of the 'Total Assets' row under 'Consolidated Assets'\n",
    "    total_assets = last_date_assets.loc[4, 'Consolidated Assets']\n",
    "\n",
    "    # Map the values of 'last_date_assets; to the 'treemap_df' dataframe based on the 'Bank' column\n",
    "    treemap_df['Consolidated Assets'] = treemap_df['Bank'].map(last_date_assets.set_index('Bank')['Consolidated Assets'])\n",
    "\n",
    "    # Rearrange order of the columns for the 'Parent' column to be first after the index\n",
    "    treemap_df = treemap_df[['Parent', 'Bank', 'Percentage', 'Consolidated Assets']]\n",
    "\n",
    "    # Assign the 'Total Assets' variable to the 'Consolidated Assets' column for the 'Other Banks' row under the 'Parent' column\n",
    "    treemap_df.loc[treemap_df['Parent'] == 'Other Banks', 'Consolidated Assets'] = total_assets\n",
    "     \n",
    "    #Download the treemap_df dataframe as a csv file\n",
    "    treemap_df.to_csv(os.path.join(dataset_dir, 'bank_asset_treemap.csv'))\n",
    "\n",
    "    return treemap_df\n",
    "\n",
    "treemap_df=create_treemap_df(percentage_df, big_four_pivot, dataset_dir)\n",
    "treemap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    " * 'get_file_sha' function retrieves the SHA of an existing file on GitHub to allow updates.\n",
    "\n",
    " * 'upload_to_github' function uploads a file to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code: 201 for bank_asset_line.csv\n",
      "Response: {'content': {'name': 'bank_asset_line.csv', 'path': 'datasets/bank_asset_line.csv', 'sha': '7269af6016d8aaa81193b24fcb8121c3bd151919', 'size': 10669, 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/contents/datasets/bank_asset_line.csv?ref=main', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/blob/main/datasets/bank_asset_line.csv', 'git_url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/blobs/7269af6016d8aaa81193b24fcb8121c3bd151919', 'download_url': 'https://raw.githubusercontent.com/juanchok12/Concentration-of-Banking/main/datasets/bank_asset_line.csv', 'type': 'file', '_links': {'self': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/contents/datasets/bank_asset_line.csv?ref=main', 'git': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/blobs/7269af6016d8aaa81193b24fcb8121c3bd151919', 'html': 'https://github.com/juanchok12/Concentration-of-Banking/blob/main/datasets/bank_asset_line.csv'}}, 'commit': {'sha': '835400498e607917fd86c2804b014aa191462649', 'node_id': 'C_kwDOKWo7DNoAKDgzNTQwMDQ5OGU2MDc5MTdmZDg2YzI4MDRiMDE0YWExOTE0NjI2NDk', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/commits/835400498e607917fd86c2804b014aa191462649', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/commit/835400498e607917fd86c2804b014aa191462649', 'author': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:35:15Z'}, 'committer': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:35:15Z'}, 'tree': {'sha': 'ca200cdb9fd2470477787fd2c837d500064bcd2a', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/trees/ca200cdb9fd2470477787fd2c837d500064bcd2a'}, 'message': 'Update bank_asset_line.csv', 'parents': [{'sha': 'fdb705f46ba44504c9be0af7ae7c08b8e123b8c7', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/commits/fdb705f46ba44504c9be0af7ae7c08b8e123b8c7', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/commit/fdb705f46ba44504c9be0af7ae7c08b8e123b8c7'}], 'verification': {'verified': False, 'reason': 'unsigned', 'signature': None, 'payload': None, 'verified_at': None}}}\n",
      "Response Status Code: 201 for bank_asset_scatter.csv\n",
      "Response: {'content': {'name': 'bank_asset_scatter.csv', 'path': 'datasets/bank_asset_scatter.csv', 'sha': 'f7c9802e52a088b7c8dc2ca7e09bc462757bfdc5', 'size': 20686, 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/contents/datasets/bank_asset_scatter.csv?ref=main', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/blob/main/datasets/bank_asset_scatter.csv', 'git_url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/blobs/f7c9802e52a088b7c8dc2ca7e09bc462757bfdc5', 'download_url': 'https://raw.githubusercontent.com/juanchok12/Concentration-of-Banking/main/datasets/bank_asset_scatter.csv', 'type': 'file', '_links': {'self': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/contents/datasets/bank_asset_scatter.csv?ref=main', 'git': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/blobs/f7c9802e52a088b7c8dc2ca7e09bc462757bfdc5', 'html': 'https://github.com/juanchok12/Concentration-of-Banking/blob/main/datasets/bank_asset_scatter.csv'}}, 'commit': {'sha': 'c5116274a6bf3f2b9087a3a15696a7505a23f1f7', 'node_id': 'C_kwDOKWo7DNoAKGM1MTE2Mjc0YTZiZjNmMmI5MDg3YTNhMTU2OTZhNzUwNWEyM2YxZjc', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/commits/c5116274a6bf3f2b9087a3a15696a7505a23f1f7', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/commit/c5116274a6bf3f2b9087a3a15696a7505a23f1f7', 'author': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:35:17Z'}, 'committer': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:35:17Z'}, 'tree': {'sha': '2f60833864c5178457c732afbe6cbc9d5b59582e', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/trees/2f60833864c5178457c732afbe6cbc9d5b59582e'}, 'message': 'Update bank_asset_scatter.csv', 'parents': [{'sha': '835400498e607917fd86c2804b014aa191462649', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/commits/835400498e607917fd86c2804b014aa191462649', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/commit/835400498e607917fd86c2804b014aa191462649'}], 'verification': {'verified': False, 'reason': 'unsigned', 'signature': None, 'payload': None, 'verified_at': None}}}\n",
      "Response Status Code: 201 for bank_asset_treemap.csv\n",
      "Response: {'content': {'name': 'bank_asset_treemap.csv', 'path': 'datasets/bank_asset_treemap.csv', 'sha': 'f503acd03be25d6b8a13e7fe4808d96fc4c571f6', 'size': 271, 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/contents/datasets/bank_asset_treemap.csv?ref=main', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/blob/main/datasets/bank_asset_treemap.csv', 'git_url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/blobs/f503acd03be25d6b8a13e7fe4808d96fc4c571f6', 'download_url': 'https://raw.githubusercontent.com/juanchok12/Concentration-of-Banking/main/datasets/bank_asset_treemap.csv', 'type': 'file', '_links': {'self': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/contents/datasets/bank_asset_treemap.csv?ref=main', 'git': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/blobs/f503acd03be25d6b8a13e7fe4808d96fc4c571f6', 'html': 'https://github.com/juanchok12/Concentration-of-Banking/blob/main/datasets/bank_asset_treemap.csv'}}, 'commit': {'sha': 'efdb195cff3df187c071412a80a237545fe4e710', 'node_id': 'C_kwDOKWo7DNoAKGVmZGIxOTVjZmYzZGYxODdjMDcxNDEyYTgwYTIzNzU0NWZlNGU3MTA', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/commits/efdb195cff3df187c071412a80a237545fe4e710', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/commit/efdb195cff3df187c071412a80a237545fe4e710', 'author': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:35:17Z'}, 'committer': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:35:17Z'}, 'tree': {'sha': 'fec085e1967b4ebe41f06bb2f100f59e7ab9c5d2', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/trees/fec085e1967b4ebe41f06bb2f100f59e7ab9c5d2'}, 'message': 'Update bank_asset_treemap.csv', 'parents': [{'sha': 'c5116274a6bf3f2b9087a3a15696a7505a23f1f7', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/commits/c5116274a6bf3f2b9087a3a15696a7505a23f1f7', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/commit/c5116274a6bf3f2b9087a3a15696a7505a23f1f7'}], 'verification': {'verified': False, 'reason': 'unsigned', 'signature': None, 'payload': None, 'verified_at': None}}}\n",
      "Response Status Code: 201 for bank_asset_percentage.xlsx\n",
      "Response: {'content': {'name': 'bank_asset_percentage.xlsx', 'path': 'datasets/bank_asset_percentage.xlsx', 'sha': 'f3071cf436f255ca49134e1f62eb8a01e0592c5a', 'size': 8623, 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/contents/datasets/bank_asset_percentage.xlsx?ref=main', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/blob/main/datasets/bank_asset_percentage.xlsx', 'git_url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/blobs/f3071cf436f255ca49134e1f62eb8a01e0592c5a', 'download_url': 'https://raw.githubusercontent.com/juanchok12/Concentration-of-Banking/main/datasets/bank_asset_percentage.xlsx', 'type': 'file', '_links': {'self': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/contents/datasets/bank_asset_percentage.xlsx?ref=main', 'git': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/blobs/f3071cf436f255ca49134e1f62eb8a01e0592c5a', 'html': 'https://github.com/juanchok12/Concentration-of-Banking/blob/main/datasets/bank_asset_percentage.xlsx'}}, 'commit': {'sha': '0dbe487e62822f0187f12dc88539f9da917d0e34', 'node_id': 'C_kwDOKWo7DNoAKDBkYmU0ODdlNjI4MjJmMDE4N2YxMmRjODg1MzlmOWRhOTE3ZDBlMzQ', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/commits/0dbe487e62822f0187f12dc88539f9da917d0e34', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/commit/0dbe487e62822f0187f12dc88539f9da917d0e34', 'author': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:35:19Z'}, 'committer': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:35:19Z'}, 'tree': {'sha': '185bd52c9d008ba699b925b9bdd0c6e57376119d', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/trees/185bd52c9d008ba699b925b9bdd0c6e57376119d'}, 'message': 'Update bank_asset_percentage.xlsx', 'parents': [{'sha': 'efdb195cff3df187c071412a80a237545fe4e710', 'url': 'https://api.github.com/repos/juanchok12/Concentration-of-Banking/git/commits/efdb195cff3df187c071412a80a237545fe4e710', 'html_url': 'https://github.com/juanchok12/Concentration-of-Banking/commit/efdb195cff3df187c071412a80a237545fe4e710'}], 'verification': {'verified': False, 'reason': 'unsigned', 'signature': None, 'payload': None, 'verified_at': None}}}\n",
      "Response Status Code: 201 for concatenated_df.csv\n",
      "Response: {'content': {'name': 'concatenated_df.csv', 'path': 'deploy/concatenated_df.csv', 'sha': '507a4187631fa0843ddff62e1c9038cdf0387be8', 'size': 25225339, 'url': 'https://api.github.com/repos/juanchok12/Consoldiated-Assets-for-Banks-and-AI/contents/deploy/concatenated_df.csv?ref=main', 'html_url': 'https://github.com/juanchok12/Consoldiated-Assets-for-Banks-and-AI/blob/main/deploy/concatenated_df.csv', 'git_url': 'https://api.github.com/repos/juanchok12/Consoldiated-Assets-for-Banks-and-AI/git/blobs/507a4187631fa0843ddff62e1c9038cdf0387be8', 'download_url': 'https://raw.githubusercontent.com/juanchok12/Consoldiated-Assets-for-Banks-and-AI/main/deploy/concatenated_df.csv', 'type': 'file', '_links': {'self': 'https://api.github.com/repos/juanchok12/Consoldiated-Assets-for-Banks-and-AI/contents/deploy/concatenated_df.csv?ref=main', 'git': 'https://api.github.com/repos/juanchok12/Consoldiated-Assets-for-Banks-and-AI/git/blobs/507a4187631fa0843ddff62e1c9038cdf0387be8', 'html': 'https://github.com/juanchok12/Consoldiated-Assets-for-Banks-and-AI/blob/main/deploy/concatenated_df.csv'}}, 'commit': {'sha': '85335a35570ed870134eed1ba94ba7621332e0d3', 'node_id': 'C_kwDOMtaWh9oAKDg1MzM1YTM1NTcwZWQ4NzAxMzRlZWQxYmE5NGJhNzYyMTMzMmUwZDM', 'url': 'https://api.github.com/repos/juanchok12/Consoldiated-Assets-for-Banks-and-AI/git/commits/85335a35570ed870134eed1ba94ba7621332e0d3', 'html_url': 'https://github.com/juanchok12/Consoldiated-Assets-for-Banks-and-AI/commit/85335a35570ed870134eed1ba94ba7621332e0d3', 'author': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:37:23Z'}, 'committer': {'name': 'Juancho', 'email': '116334702+juanchok12@users.noreply.github.com', 'date': '2025-02-12T19:37:23Z'}, 'tree': {'sha': 'f350492e51abec6fd4e41c608074ef8d405fd0c7', 'url': 'https://api.github.com/repos/juanchok12/Consoldiated-Assets-for-Banks-and-AI/git/trees/f350492e51abec6fd4e41c608074ef8d405fd0c7'}, 'message': 'Update concatenated_df.csv', 'parents': [{'sha': '1dc78e460e32a0029ec1f207ada2a08f15d029d2', 'url': 'https://api.github.com/repos/juanchok12/Consoldiated-Assets-for-Banks-and-AI/git/commits/1dc78e460e32a0029ec1f207ada2a08f15d029d2', 'html_url': 'https://github.com/juanchok12/Consoldiated-Assets-for-Banks-and-AI/commit/1dc78e460e32a0029ec1f207ada2a08f15d029d2'}], 'verification': {'verified': False, 'reason': 'unsigned', 'signature': None, 'payload': None, 'verified_at': None}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_file_sha(repo, path, filename, token):\n",
    "    \"\"\"\n",
    "    Retrieve the SHA of an existing file in the specified repository/folder.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{repo}/contents/{path}/{filename}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('sha')\n",
    "    return None\n",
    "\n",
    "def upload_to_github(repo, path, token, file_path):\n",
    "    \"\"\"\n",
    "    Uploads (or updates) a single file to the specified repository and path.\n",
    "    \"\"\"\n",
    "    # Use os.path.basename for cross-platform filename extraction\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_sha = get_file_sha(repo, path, filename, token)  # Get SHA if file exists\n",
    "    api_url = f\"https://api.github.com/repos/{repo}/contents/{path}/{filename}\"\n",
    "    \n",
    "    # Read and encode file content\n",
    "    with open(file_path, 'rb') as file:\n",
    "        file_content = base64.b64encode(file.read()).decode('utf-8')\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {token}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "    }\n",
    "    data = {\n",
    "        \"message\": f\"Update {filename}\",\n",
    "        \"content\": file_content,\n",
    "        \"branch\": \"main\",\n",
    "    }\n",
    "    if file_sha:  # Include SHA if updating an existing file\n",
    "        data['sha'] = file_sha\n",
    "    \n",
    "    response = requests.put(api_url, headers=headers, data=json.dumps(data))\n",
    "    print(f\"Response Status Code: {response.status_code} for {filename}\")\n",
    "    print(\"Response:\", response.json())\n",
    "\n",
    "def upload_multiple_files(repo, path, token, file_paths):\n",
    "    \"\"\"\n",
    "    Loops through a list of file paths and uploads each file to the specified repository and path.\n",
    "    \"\"\"\n",
    "    for file_path in file_paths:\n",
    "        upload_to_github(repo, path, token, file_path)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# 1. Upload multiple dataset files to one repository:\n",
    "file_paths = [\n",
    "    './datasets/bank_asset_line.csv',\n",
    "    './datasets/bank_asset_scatter.csv',\n",
    "    './datasets/bank_asset_treemap.csv',\n",
    "    './datasets/bank_asset_percentage.xlsx',\n",
    "]\n",
    "upload_multiple_files('juanchok12/Concentration-of-Banking', 'datasets', 'key_github_here', file_paths)\n",
    "\n",
    "# 2. Upload the concatenated_df.csv file to a different repository and folder (\"deploy\"):\n",
    "# Define the path as a string rather than a list.\n",
    "concatenated_file_path = './datasets/concatenated_df.csv'\n",
    "upload_to_github('juanchok12/Consoldiated-Assets-for-Banks-and-AI', 'deploy', 'key_github_here', concatenated_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
